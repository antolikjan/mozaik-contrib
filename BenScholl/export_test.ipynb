{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3b43eed3-ae49-4a23-89a1-40d721289f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DSV info:\n",
      "  DSV info:\n",
      "  DSV info:\n",
      "  DSV info:\n",
      "  DSV info:\n",
      "  DSV info:\n",
      "  DSV info:\n",
      "     Number of recordings: 60012\n",
      "     Number of recordings: 60012\n",
      "     Number of recordings: 60012\n",
      "     Number of recordings: 60012\n",
      "     Number of recordings: 60012\n",
      "     Number of recordings: 60012\n",
      "     Number of recordings: 60012\n",
      "       InternalStimulus : 12\n",
      "       InternalStimulus : 12\n",
      "       InternalStimulus : 12\n",
      "       InternalStimulus : 12\n",
      "       InternalStimulus : 12\n",
      "       InternalStimulus : 12\n",
      "       InternalStimulus : 12\n",
      "       StaticImage : 60000\n",
      "       StaticImage : 60000\n",
      "       StaticImage : 60000\n",
      "       StaticImage : 60000\n",
      "       StaticImage : 60000\n",
      "       StaticImage : 60000\n",
      "       StaticImage : 60000\n",
      "     Number of ADS: 0\n",
      "     Number of ADS: 0\n",
      "     Number of ADS: 0\n",
      "     Number of ADS: 0\n",
      "     Number of ADS: 0\n",
      "     Number of ADS: 0\n",
      "     Number of ADS: 0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from mozaik.storage.datastore import PickledDataStore\n",
    "from mozaik.controller import Global\n",
    "from parameters import ParameterSet\n",
    "from mozaik.controller import setup_logging\n",
    "from mozaik.tools.export_to_hdf5 import *\n",
    "\n",
    "def get_datastore(root):\n",
    "    Global.root_directory = root\n",
    "    setup_logging()\n",
    "    \n",
    "    datastore = PickledDataStore(\n",
    "        load=True,\n",
    "        parameters=ParameterSet({\"root_directory\": root, \"store_stimuli\": None}),\n",
    "        replace=True,\n",
    "    )\n",
    "    return datastore\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    simulation_path = sys.argv[1]\n",
    "    datastore = get_datastore('/projects/urbanosz/multitrial/20241226-154716[param_nat_img.defaults]CombinationParamSearch{experiments.images_dir:[0],experiments.num_skipped_images:[200000],experiments.num_images:[50],experiments.num_trials:[100]}/V1_model_Dataset_ParameterSearch_____images_dir:0_num_images:50_num_skipped_images:200000_num_trials:100')\n",
    "    datastore.print_content()\n",
    "    #export_from_datastore_to_hdf5(data_store=datastore, st_name='StaticImage', data_type='mean_rates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "85c1a1cf-7cd7-4a4a-9287-9d8a01077c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DSV info:\n",
      "  DSV info:\n",
      "  DSV info:\n",
      "  DSV info:\n",
      "  DSV info:\n",
      "  DSV info:\n",
      "  DSV info:\n",
      "     Number of recordings: 10002\n",
      "     Number of recordings: 10002\n",
      "     Number of recordings: 10002\n",
      "     Number of recordings: 10002\n",
      "     Number of recordings: 10002\n",
      "     Number of recordings: 10002\n",
      "     Number of recordings: 10002\n",
      "       InternalStimulus : 2\n",
      "       InternalStimulus : 2\n",
      "       InternalStimulus : 2\n",
      "       InternalStimulus : 2\n",
      "       InternalStimulus : 2\n",
      "       InternalStimulus : 2\n",
      "       InternalStimulus : 2\n",
      "       StaticImage : 10000\n",
      "       StaticImage : 10000\n",
      "       StaticImage : 10000\n",
      "       StaticImage : 10000\n",
      "       StaticImage : 10000\n",
      "       StaticImage : 10000\n",
      "       StaticImage : 10000\n",
      "     Number of ADS: 0\n",
      "     Number of ADS: 0\n",
      "     Number of ADS: 0\n",
      "     Number of ADS: 0\n",
      "     Number of ADS: 0\n",
      "     Number of ADS: 0\n",
      "     Number of ADS: 0\n"
     ]
    }
   ],
   "source": [
    "dsv = param_filter_query(datastore,sheet_name='V1_Exc_L2/3')\n",
    "\n",
    "dsv.print_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bd420278-0111-4023-b5ee-36f504241078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Loading 33\n",
      "1\n",
      "Loading 34\n",
      "2\n",
      "Loading 35\n",
      "3\n",
      "Loading 36\n",
      "4\n",
      "Loading 37\n",
      "5\n",
      "Loading 38\n",
      "6\n",
      "Loading 39\n",
      "7\n",
      "Loading 40\n",
      "8\n",
      "Loading 41\n",
      "9\n",
      "Loading 42\n"
     ]
    }
   ],
   "source": [
    "import cProfile\n",
    "def aaa(dsv):\n",
    "    segs = dsv.get_segments()\n",
    "    for i in range(0,10): \n",
    "        print(i)\n",
    "        segs[i].get_spiketrains()\n",
    "cProfile.run('aaa(dsv)', 'restats')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3eb68cad-d8e5-4f7c-85f7-64b7cc6f9fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sheet_data_and_save_to_h5py(stims, segs, varying_stim_params, data_type, stimuli_subgroup, sheet_name, cut_start, cut_end):\n",
    "        # Get data to export\n",
    "        logging.info(f\"Extracting {data_type} data from {len(segs)} segments in sheet {sheet_name}\")\n",
    "        data = []\n",
    "        for seg in segs:\n",
    "            if data_type == 'mean_rates':\n",
    "                if cut_start is not None:\n",
    "                    cut_start = Quantity(cut_start, 'ms')\n",
    "                if cut_end is not None:\n",
    "                    cut_end = Quantity(cut_end, 'ms')\n",
    "                data.append(seg.mean_rates(start=cut_start, end=cut_end))\n",
    "            elif data_type == 'spiketrains':\n",
    "                data.append([spiketrain.time_slice(cut_start, cut_end).magnitude for spiketrain in seg.get_spiketrains()])\n",
    "            else:\n",
    "                raise ValueError(\"Invalid data type\")\n",
    "            seg.release()\n",
    "\n",
    "        data = np.array(data)\n",
    "\n",
    "        # Reorder stimuli and data in tensors whose number of dimensions corresponds to the number of varying parameters\n",
    "        stims_sorted, data_sorted = reorder_lists(stims, data, varying_stim_params.keys())\n",
    "\n",
    "        # fix 2\n",
    "        gc.collect()\n",
    "        # stims_tensor = np.reshape(stims_sorted, [len(varying_stim_params[param]) for param in varying_stim_params.keys()])\n",
    "        params_dims = [len(varying_stim_params[param]) for param in varying_stim_params.keys()]\n",
    "\n",
    "        data_tensor = np.reshape(np.array(data_sorted).flatten(), [*params_dims, -1])\n",
    "\n",
    "        # Add dataset to the stimuli subgroup\n",
    "        sheet_name_cleaned = sheet_name.replace('/', '')\n",
    "        if data_type =='spiketrains':\n",
    "            dset = stimuli_subgroup.create_dataset(sheet_name_cleaned, shape=data_tensor.shape, dtype=h5py.special_dtype(vlen=np.dtype('float')))\n",
    "            dset[:] = data_tensor\n",
    "        else:\n",
    "            stimuli_subgroup.create_dataset(sheet_name_cleaned, data=data_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41815de1-31f0-437c-8d7a-7175c7eca8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_from_datastore_to_hdf5(data_store, st_name, data_type, start_time=None, stop_time=None, time_windows_size=None, path_to_save_hdf5=None):\n",
    "    \"\"\"\n",
    "    Export data from a Mozaik datastore to a HDF5 file with a standardized structure.\n",
    "\n",
    "    Parameters:\n",
    "        data_store (DataStore): The Mozaik datastore containing simulation results\n",
    "        st_name (str): The name of the stimulus to be exported\n",
    "        data_type (str): The type of data to export. Options:\n",
    "                        - 'mean_rates': Average firing rates\n",
    "                        - 'spiketrains': Raw spike times\n",
    "        start_time (float, optional): Start time (ms) for data extraction\n",
    "        stop_time (float, optional): End time (ms) for data extraction\n",
    "\n",
    "    Notes:\n",
    "        Creates an HDF5 file with the following structure:\n",
    "        - Root attributes: default_parameters, sim_info, sheets, data_type, st_name, recorders\n",
    "        - Model groups: Contains parameter sets and stimulus data\n",
    "        - Stimulus groups: Contains:\n",
    "            - Constant/varying parameter information\n",
    "            - Neural response data for each sheet\n",
    "            - Stimulus data and indices\n",
    "            - Data cut timing information\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def serialize_parameters(params):\n",
    "        serialized = {}\n",
    "        for key, value in params.items():\n",
    "            if isinstance(value, dict):\n",
    "                serialized[key] = serialize_parameters(value)\n",
    "            elif isinstance(value, PyNNDistribution):\n",
    "                serialized[key] = str(value)\n",
    "            else:\n",
    "                serialized[key] = value\n",
    "        return serialized\n",
    "    \n",
    "    def create_hdf5_structure(hf, sim_info, default_parameters, modified_parameters, recorders, experimental_protocols, st_name, varying_stim_params, constant_stim_params, data_type, start_time, stop_time):\n",
    "        # Add default parameters and info as metadata to the group\n",
    "        hf.attrs['default_parameters'] = str(serialize_parameters(default_parameters))\n",
    "        hf.attrs['sim_info'] = str(sim_info)\n",
    "        hf.attrs['data_type'] = data_type\n",
    "        hf.attrs['st_name'] = st_name\n",
    "        hf.attrs['recorders'] = str(recorders)\n",
    "        hf.attrs['experimental_protocols'] = str(experimental_protocols)\n",
    "\n",
    "        # Create a subgroup based on modified parameters\n",
    "        if modified_parameters:\n",
    "            model_subgroup_name = str(modified_parameters)\n",
    "        else:\n",
    "            model_subgroup_name = \"default\"\n",
    "        model_subgroup = hf.create_group(model_subgroup_name)\n",
    "\n",
    "        # Merge modified parameters into default parameters\n",
    "        merged_parameters = default_parameters.copy()\n",
    "        merged_parameters.update(modified_parameters)\n",
    "\n",
    "        # Add merged parameters as metadata to the model_subgroup\n",
    "        model_subgroup.attrs['parameters'] = str(serialize_parameters(merged_parameters))\n",
    "        logging.info(f\"Model subgroup '{model_subgroup_name}' created with merged parameters as metadata.\")\n",
    "\n",
    "        # Create a stimuli subgroup\n",
    "        stimuli_subgroup = model_subgroup.create_group(st_name)\n",
    "        logging.info(f\"Datasets subgroup created under 'stimuli' in '{model_subgroup_name}'.\")\n",
    "\n",
    "        # Add varying parameters as metadata to the stimuli subgroup\n",
    "        stimuli_subgroup.attrs['varying_parameters'] = list(varying_stim_params.keys())\n",
    "        for param_name, param_values in varying_stim_params.items():\n",
    "            stimuli_subgroup.attrs[f'{param_name}'] = param_values\n",
    "        stimuli_subgroup.attrs['data_dimensions'] = [len(varying_stim_params[param]) for param in varying_stim_params.keys()]\n",
    "\n",
    "        # Add constant parameters as metadata to the stimuli subgroup\n",
    "        stimuli_subgroup.attrs['constant_parameters'] = list(constant_stim_params.keys())\n",
    "        for param_name, param_values in constant_stim_params.items():\n",
    "            stimuli_subgroup.attrs[f'{param_name}'] = str(param_values) if param_values is not None else \"None\"\n",
    "\n",
    "        # Add data related metadata to the stimuli subgroup\n",
    "        stimuli_subgroup.attrs['data_type'] = data_type\n",
    "        stimuli_subgroup.attrs['data_start_time'] = str(start_time) if start_time is not None else \"None\"\n",
    "        stimuli_subgroup.attrs['data_stop_time'] = str(stop_time) if stop_time is not None else \"None\"\n",
    "\n",
    "        return stimuli_subgroup\n",
    "\n",
    "    def get_segments_and_stimuli_and_constant_and_varying_parameters(data_store, sheet_name, st_name):\n",
    "        # Get segments and stimuli\n",
    "        dsv = param_filter_query(data_store, st_name=st_name, sheet_name=sheet_name)\n",
    "        segs = dsv.get_segments()\n",
    "        stims = [MozaikParametrized.idd(seg.annotations['stimulus']) for seg in segs]\n",
    "        segs_pre =  dsv.get_segments(null=True)\n",
    "        segs_post = [*segs_pre[1:], param_filter_query(data_store, st_name='InternalStimulus', sheet_name=sheet_name).get_segments()[-1]]\n",
    "        segs = list(zip(segs_pre, segs, segs_post))\n",
    "\n",
    "        # Get varying parameters\n",
    "        constant_stim_params, varying_stim_params = classify_stimulus_parameters_into_constant_and_varying(stims)  # alternative: params = OrderedDict((param, sorted(list(parameter_value_list(stims, param)))) for param in varying_parameters(stims))\n",
    "\n",
    "        # Assert all possible combinations of varying stimulus parameters are present in the stimuli in the list stims and that there are no duplicates\n",
    "        assert len(stims) == np.prod([len(varying_stim_params[param]) for param in varying_stim_params.keys()]), \"Number of stimuli does not match the product of the number of varying parameter values\"\n",
    "        assert len(set([str(stim) for stim in stims])) == len(stims), \"There are duplicate stimuli\"\n",
    "        return segs, stims, constant_stim_params, varying_stim_params\n",
    "\n",
    "    def extract_sheet_data_and_save_to_h5py(stims, segs, varying_stim_params, data_type, stimuli_subgroup, sheet_name, start_time, stop_time, time_windows_size):\n",
    "        # Get data to export\n",
    "        logging.info(f\"Extracting {data_type} data from {len(segs)} segments in sheet {sheet_name}\")\n",
    "\n",
    "        # define start_time and stop_time if not provided\n",
    "        if start_time is None:\n",
    "            start_time = 0\n",
    "        if stop_time is None:\n",
    "            stop_time = stims[0].duration\n",
    "\n",
    "        # define time_windows\n",
    "        if time_windows_size == None: \n",
    "            time_windows = [(start_time, stop_time)]\n",
    "            time_windows_size = stop_time - start_time\n",
    "        else:\n",
    "            assert float(stop_time-start_time) % (time_windows_size) == 0, \"time_windows_size must be a multiple of the time_stop - time_start\"\n",
    "            time_windows = list(zip(np.arange(start_time, stop_time, time_windows_size), np.arange(start_time+time_windows_size, stop_time+time_windows_size, time_windows_size)))\n",
    "\n",
    "        data = []\n",
    "        for seg in segs:  \n",
    "            concatenated_seg = concatenate_segments_with_offsets(*seg)\n",
    "            if data_type == 'spike_counts':\n",
    "                data.append(count_spikes_in_multiple_windows(concatenated_seg, time_windows))  \n",
    "            elif data_type == 'mean_rates':\n",
    "                data.append(count_spikes_in_multiple_windows(concatenated_seg, time_windows)/float(time_windows_size) * 1000)  \n",
    "            elif data_type == 'spiketrains':\n",
    "                data.append([spiketrain.time_slice(start_time, stop_time).magnitude for spiketrain in concatenated_seg.spiketrains])\n",
    "            else:\n",
    "                raise ValueError(\"Invalid data type\")\n",
    "        if data_type == 'spiketrains':\n",
    "            data = np.array(data, dtype=object)\n",
    "        else:\n",
    "            data = np.stack(data)\n",
    "\n",
    "\n",
    "        # Reorder stimuli and data in tensors whose number of dimensions corresponds to the number of varying parameters\n",
    "        _, data_sorted = reorder_lists(stims, data, varying_stim_params.keys())\n",
    "\n",
    "        params_dims = [len(varying_stim_params[param]) for param in varying_stim_params.keys()]\n",
    "        # reshape data to match the dimensions of varying parameters\n",
    "        data_tensor = np.reshape(np.array(data_sorted).flatten(), [*params_dims, *data_sorted[0].shape])\n",
    "  \n",
    "        # Add dataset to the stimuli subgroup\n",
    "        sheet_name_cleaned = sheet_name.replace('/', '')\n",
    "        if data_type =='spiketrains':\n",
    "            dset = stimuli_subgroup.create_dataset(sheet_name_cleaned, shape=data_tensor.shape, dtype=h5py.special_dtype(vlen=np.dtype('float')))\n",
    "            dset[:] = data_tensor\n",
    "        else:\n",
    "            stimuli_subgroup.create_dataset(sheet_name_cleaned, data=data_tensor)\n",
    "\n",
    "    def add_stimuli_dataset(stimuli_subgroup, stims, varying_stim_params, ds):\n",
    "        logging.info(f\"Adding stimuli dataset to {stimuli_subgroup.name}\")\n",
    "        # Reorder stimuli and reshape to match the dimensions of varying parameters\n",
    "        reordered_stims, _ = reorder_lists(stims, [str(s) for s in stims], varying_stim_params.keys()) \n",
    "        reordered_stims = np.array(reordered_stims).reshape([len(varying_stim_params[param]) for param in varying_stim_params.keys()])\n",
    "\n",
    "        # Identify which dimension corresponds to trial\n",
    "        trial_dim = None\n",
    "        for i, param in enumerate(varying_stim_params.keys()):\n",
    "            if param == 'trial':\n",
    "                trial_dim = i\n",
    "                stimuli_subgroup.attrs['trial_dim'] = trial_dim\n",
    "                break\n",
    "\n",
    "        # Drop the trial dimension by selecting the first element along it\n",
    "        if trial_dim != None:\n",
    "            reordered_stims = np.take(reordered_stims, 0, axis=trial_dim)\n",
    "        # create index\n",
    "        reordered_stims_flat = reordered_stims.flatten()\n",
    "        reordered_stims_idx = np.arange(len(reordered_stims_flat)).reshape(reordered_stims.shape)\n",
    "        # reinsert trial dimension\n",
    "        if trial_dim != None:\n",
    "            reordered_stims_idx = np.expand_dims(reordered_stims_idx, axis=trial_dim).repeat(len(varying_stim_params['trial']), axis=trial_dim)\n",
    "\n",
    "        sensory_stim = np.array(ds.get_sensory_stimulus([str(s) for s in reordered_stims_flat])).squeeze()\n",
    "        stimuli_subgroup.create_dataset('stimuli', data=sensory_stim)   \n",
    "        stimuli_subgroup.create_dataset('stimuli_idx', data=reordered_stims_idx)\n",
    "\n",
    "        # Save the stimulus_name dataset\n",
    "        stimulus_names = np.array([str(stim) for stim in stims], dtype='S')\n",
    "        stimuli_subgroup.create_dataset('stimulus_name', data=stimulus_names, dtype=h5py.string_dtype(encoding='utf-8'))\n",
    "\n",
    "    ############################################################################################\n",
    "    ## Create an HDF5 file (main function)\n",
    "    ############################################################################################\n",
    "    base_folder = data_store.parameters['root_directory']\n",
    "    if path_to_save_hdf5 is None:\n",
    "        path_to_save_hdf5 = os.path.join(base_folder, 'exported_data.h5')\n",
    "    assert path_to_save_hdf5.endswith('.h5'), \"path_to_save_hdf5 must end with .h5\"\n",
    "    os.makedirs(os.path.dirname(path_to_save_hdf5), exist_ok=True)\n",
    "    with h5py.File(path_to_save_hdf5, 'w') as hf:\n",
    "        # Get model info and parameters\n",
    "        modified_parameters, default_parameters, info, recorders, experimental_protocols = get_model_info_and_parameters(base_folder, separate_modified_params=True)\n",
    "        sheets =  data_store.sheets() \n",
    "        hf.attrs['sheets'] = [sheet.replace('/', '') for sheet in sheets]\n",
    "        \n",
    "        # Iterate over all sheets, extract data and save to h5py\n",
    "        for i, sheet_name in enumerate(sheets):\n",
    "            logging.info(f\"Processing sheet {sheet_name}\")\n",
    "            # Get segments and stimuli and constant and varying parameters for the current sheet\n",
    "            segs, stims, constant_stim_params, varying_stim_params = get_segments_and_stimuli_and_constant_and_varying_parameters(data_store=data_store, sheet_name=sheet_name, st_name=st_name)\n",
    "            \n",
    "            # Create HDF5 structure for the first sheet\n",
    "            if i == 0:\n",
    "                stimuli_subgroup = create_hdf5_structure(\n",
    "                    hf, info, default_parameters, modified_parameters, recorders, experimental_protocols, st_name,\n",
    "                    varying_stim_params, constant_stim_params, data_type, start_time, stop_time\n",
    "                )\n",
    "\n",
    "            if len(segs[0][1].get_spiketrains()) == 0: # maybe there is a better way to check if there are neurons recorded in the sheet\n",
    "                logging.warning(f\"No neurons recorded in sheet {sheet_name}\")\n",
    "                continue\n",
    "            \n",
    "            # # Extract data and save to h5py\n",
    "            extract_sheet_data_and_save_to_h5py(stims, segs, varying_stim_params, data_type, stimuli_subgroup, sheet_name, start_time, stop_time, time_windows_size) \n",
    "\n",
    "        # Add stimuli dataset\n",
    "        add_stimuli_dataset(stimuli_subgroup, stims, varying_stim_params, data_store)\n",
    "\n",
    "    logging.info(f\"HDF5 file created with default parameters, info, list of sheets as metadata, stimuli subgroup, and datasets subgroup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0bfbab-aa94-4a43-9dff-5372473f9ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_from_datastore_to_hdf5(data_store=datastore, st_name='StaticImage', data_type='mean_rates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "994c2236-b7ad-41ff-b4cb-5328d3f27691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan 14 14:11:39 2025    restats\n",
      "\n",
      "         170375176 function calls (156848291 primitive calls) in 280.012 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 330 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "       95    0.786    0.008  413.772    4.355 /usr/lib/python3.12/asyncio/base_events.py:1910(_run_once)\n",
      "        1    0.000    0.000  306.092  306.092 {built-in method builtins.exec}\n",
      "        1    0.000    0.000  306.090  306.090 /tmp/ipykernel_188518/4068454617.py:2(aaa)\n",
      "       10    0.000    0.000  306.085   30.609 /home/antolikjan/virt_env/mozaikluca/lib/python3.12/site-packages/mozaik/storage/neo_neurotools_wrapper.py:47(get_spiketrains)\n",
      "       10    0.003    0.000  266.664   26.666 /home/antolikjan/virt_env/mozaikluca/lib/python3.12/site-packages/mozaik/storage/neo_neurotools_wrapper.py:263(load_full)\n",
      "       10    7.916    0.792  253.618   25.362 {built-in method _pickle.load}\n",
      "   375000    1.998    0.000  245.901    0.001 /home/antolikjan/virt_env/mozaikluca/lib/python3.12/site-packages/neo/core/spiketrain.py:96(_new_spiketrain)\n",
      "   375000    4.355    0.000  233.476    0.001 /home/antolikjan/virt_env/mozaikluca/lib/python3.12/site-packages/neo/core/spiketrain.py:260(__new__)\n",
      "  3756420    7.166    0.000  202.140    0.000 /home/antolikjan/virt_env/mozaikluca/lib/python3.12/site-packages/quantities/registry.py:62(__getitem__)\n",
      "  1876400    3.991    0.000  183.118    0.000 /home/antolikjan/virt_env/mozaikluca/lib/python3.12/site-packages/quantities/quantity.py:118(__new__)\n",
      "  3756420   20.469    0.000  175.976    0.000 /home/antolikjan/virt_env/mozaikluca/lib/python3.12/site-packages/quantities/registry.py:18(__getitem__)\n",
      "   376210    1.537    0.000  173.935    0.000 /home/antolikjan/virt_env/mozaikluca/lib/python3.12/site-packages/quantities/quantity.py:437(__eq__)\n",
      "16910014/5635804    4.590    0.000  171.118    0.000 {built-in method builtins.hash}\n",
      "752420/376210    1.481    0.000  170.959    0.000 /home/antolikjan/virt_env/mozaikluca/lib/python3.12/site-packages/quantities/quantity.py:202(rescale)\n",
      "  2254840    5.882    0.000  169.393    0.000 /home/antolikjan/virt_env/mozaikluca/lib/python3.12/site-packages/quantities/dimensionality.py:59(__hash__)\n",
      "   752420    0.439    0.000  142.881    0.000 /home/antolikjan/virt_env/mozaikluca/lib/python3.12/site-packages/quantities/dimensionality.py:197(__eq__)\n",
      "752420/376210    2.413    0.000  135.880    0.000 /home/antolikjan/virt_env/mozaikluca/lib/python3.12/site-packages/quantities/quantity.py:253(astype)\n",
      "    94/92   43.146    0.459   71.321    0.775 /usr/lib/python3.12/selectors.py:451(select)\n",
      "  3756420   38.483    0.000   64.601    0.000 {built-in method builtins.eval}\n",
      "  1125190    1.116    0.000   47.342    0.000 /home/antolikjan/virt_env/mozaikluca/lib/python3.12/site-packages/quantities/quantity.py:785(_reconstruct_quantity)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x716baa1a9b20>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pstats\n",
    "from pstats import SortKey\n",
    "p = pstats.Stats('restats')\n",
    "p.sort_stats(SortKey.CUMULATIVE).print_stats(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d0dfee6-6687-4fcb-bc0a-e14f03d1a84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neo \n",
    "import quantities as qt \n",
    "import cProfile\n",
    "\n",
    "def aaa():\n",
    "    for i in range(3600000):\n",
    "        neo.SpikeTrain(times=[]*qt.ms,t_stop=100)\n",
    "\n",
    "cProfile.run('aaa()', 'restats_sp')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fb053449-2910-4ba8-91c1-2a144e0697d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan 14 14:38:44 2025    restats\n",
      "\n",
      "         170375161 function calls (156848380 primitive calls) in 292.885 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 295 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "      2/1    0.000    0.000  316.440  316.440 {built-in method builtins.exec}\n",
      "        1    0.000    0.000  316.438  316.438 /tmp/ipykernel_188518/4068454617.py:2(aaa)\n",
      "       10    0.000    0.000  316.433   31.643 /home/antolikjan/virt_env/mozaikluca/lib/python3.12/site-packages/mozaik/storage/neo_neurotools_wrapper.py:47(get_spiketrains)\n",
      "       10    8.812    0.881  288.035   28.804 {built-in method _pickle.load}\n",
      "   375000    2.147    0.000  251.341    0.001 /home/antolikjan/virt_env/mozaikluca/lib/python3.12/site-packages/neo/core/spiketrain.py:96(_new_spiketrain)\n",
      "   375000    4.542    0.000  238.187    0.001 /home/antolikjan/virt_env/mozaikluca/lib/python3.12/site-packages/neo/core/spiketrain.py:260(__new__)\n",
      "       10    0.019    0.002  235.323   23.532 /home/antolikjan/virt_env/mozaikluca/lib/python3.12/site-packages/mozaik/storage/neo_neurotools_wrapper.py:263(load_full)\n",
      "  3756420    7.697    0.000  199.844    0.000 /home/antolikjan/virt_env/mozaikluca/lib/python3.12/site-packages/quantities/registry.py:62(__getitem__)\n",
      "  3756420   22.102    0.000  184.596    0.000 /home/antolikjan/virt_env/mozaikluca/lib/python3.12/site-packages/quantities/registry.py:18(__getitem__)\n",
      "  1876400    4.201    0.000  178.132    0.000 /home/antolikjan/virt_env/mozaikluca/lib/python3.12/site-packages/quantities/quantity.py:118(__new__)\n",
      "   376210    1.640    0.000  167.894    0.000 /home/antolikjan/virt_env/mozaikluca/lib/python3.12/site-packages/quantities/quantity.py:437(__eq__)\n",
      "16910014/5635802    4.861    0.000  165.492    0.000 {built-in method builtins.hash}\n",
      "752420/376210    1.556    0.000  164.753    0.000 /home/antolikjan/virt_env/mozaikluca/lib/python3.12/site-packages/quantities/quantity.py:202(rescale)\n",
      "  2254840    6.304    0.000  163.672    0.000 /home/antolikjan/virt_env/mozaikluca/lib/python3.12/site-packages/quantities/dimensionality.py:59(__hash__)\n",
      "   752420    0.441    0.000  134.542    0.000 /home/antolikjan/virt_env/mozaikluca/lib/python3.12/site-packages/quantities/dimensionality.py:197(__eq__)\n",
      "752420/376210    2.523    0.000  126.725    0.000 /home/antolikjan/virt_env/mozaikluca/lib/python3.12/site-packages/quantities/quantity.py:253(astype)\n",
      "       92    0.256    0.003   77.639    0.844 /usr/lib/python3.12/asyncio/base_events.py:1910(_run_once)\n",
      "       92   50.209    0.546   67.065    0.729 /usr/lib/python3.12/selectors.py:451(select)\n",
      "  3756420   42.830    0.000   66.427    0.000 {built-in method builtins.eval}\n",
      "      246    0.004    0.000   64.012    0.260 {built-in method time.sleep}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x716b5e9516a0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pstats\n",
    "from pstats import SortKey\n",
    "p = pstats.Stats('restats')\n",
    "p.sort_stats(SortKey.CUMULATIVE).print_stats(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bc4c1f-072c-4dd0-9672-401355ced109",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
